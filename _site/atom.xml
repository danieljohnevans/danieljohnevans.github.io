<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>danieljohnevans</title>
 <link href="http://danieljohnevans.github.io/" rel="self"/>
 <link href="http://danieljohnevans.github.io"/>
 <updated>2016-08-10T13:34:19-04:00</updated>
 <id>http://danieljohnevans.github.io</id>
 <author>
   <name></name>
   <email></email>
 </author>

 
 <entry>
   <title>Early Modern Bots</title>
   <link href="http://danieljohnevans.github.io/sdfb/bot/2016/08/04/bacon-bot/"/>
   <updated>2016-08-04T13:09:50-04:00</updated>
   <id>http://danieljohnevans.github.io/sdfb/bot/2016/08/04/bacon-bot</id>
   <content type="html">&lt;p&gt;I started at CMU earlier this spring and since joining have been working with the creative minds of the &lt;a href=&quot;http://www.sixdegreesoffrancisbacon.com/&quot;&gt;Six Degrees of Francis Bacon Project&lt;/a&gt;. These folks are mining names from the &lt;a href=&quot;http://www.oxforddnb.com/&quot;&gt;Oxford Dictionary of National Biography&lt;/a&gt; and creating an early modern social network. It’s an amazing project. I recommend checking it out if you haven’t already.&lt;/p&gt;

&lt;p&gt;Early in the project I spent admittedly too much time clicking on individuals, trying to get a sense of who they were and how they interacted. As someone who doesn’t have a background in early modernism I found that I knew most of the big names but few of the smaller ones (embarrassingly I referred to Charles V as a English king in one meeting – oops). Unfortunately my explorations quickly dead-ended because I only knew a handful of prominent individuals and their social networks tended to be exclusionary.&lt;/p&gt;

&lt;p&gt;After sketching out a few possible solutions, I realized that I  wanted to view random relationships of lesser-known figures and do it automatically. I ultimately settled on a twitter bot as twitter is something I check rather frequently and I’ve been wanting an excuse to build a bot for awhile now. The other requirement for this project was I wanted to dive back into Ruby. My Jekyll site is built with Ruby and the Six Degrees project is built on Rails so I knew it would be a useful exercise.&lt;/p&gt;

&lt;h3 id=&quot;the-bot&quot;&gt;The Bot&lt;/h3&gt;

&lt;p&gt;So the bot by itself is conceptually rather simple. It pulls a random random relationship_id from the Six Degrees of Francis Bacon project. It then looks up the individual_id associated with that relationship and prints out:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Information about that relationship while substituting the individual_id for a display name&lt;/li&gt;
  &lt;li&gt;It responds to itself with information about those individuals (birth year, death year, historical significance)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So how are these ids generated?&lt;/p&gt;

&lt;p&gt;The bot makes these calls to the database by recognizing a pattern of the url. In the Six Bacon project it follows .com/relationships/# or .com/groups/#. This number will always link back to a unique identifier provided that one exists. Furthermore simply adding .json to nearly any page will provide provide an API version of that page.&lt;/p&gt;

&lt;p&gt;I’ve provided a few examples below to help illustrate my point:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;The front end of Francis Bacon’s relationship to Anne Bacon is &lt;a href=&quot;http://www.sixdegreesoffrancisbacon.com/relationships/100009300&quot;&gt;here&lt;/a&gt;. And the back end version is &lt;a href=&quot;http://www.sixdegreesoffrancisbacon.com/relationships/100009300.json&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The front end of Francis Bacon’s profile is &lt;a href=&quot;http://www.sixdegreesoffrancisbacon.com/people/10000473&quot;&gt;here&lt;/a&gt;. The back end is &lt;a href=&quot;http://www.sixdegreesoffrancisbacon.com/people/10000473.json&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I’ve provided my code below with some basic documentation for reference:&lt;/p&gt;

&lt;p&gt;First install gems and require them. Next plug in your twitter consumer key, consumer secret, access token, access token secret. There are plenty of other guides online and I won’t go over all this here. However, if you have questions please reach out.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;require &#39;httparty&#39;
require &#39;bundler/setup&#39;
require &#39;json&#39;
require &#39;rubygems&#39;
require &#39;twitter&#39;

$client = Twitter::REST::Client.new do |config|
    config.consumer_key          = &quot;YOUR_CONSUMER_KEY&quot;
    config.consumer_secret       = &quot;YOUR_CONSUMER_SECRET&quot;
    config.access_token          = &quot;YOUR_ACCESS_TOKEN&quot;
    config.access_token_secret   = &quot;YOUR_ACCESS_TOKEN_SECRET&quot;
end
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;After the setup is completed, generate a random number and plug that number into the relationship url. I should note that while there are gaps in the relationship_ids, an end user could roughly identify the number of 100000000 as the beginning of the relationship_id and 100200719 as the end. Generating a random number in between would provide a positive hit.&lt;/p&gt;

&lt;p&gt;Furthermore, we want ruby to recognize that the output is json.&lt;/p&gt;

&lt;p&gt;Down the line we’ll need to provide a further lookup because the relationship url includes references to individual_ids. To solve this issue, we’ll need to provide a parallel lookup. This follows the same format and I’ve included the code below for simplicity.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;randos = Hash.new( rand(100000000..100200719) )

class Randomizer
    include HTTParty
    format :json

    def self.find_by_id(id)
        get(&quot;http://www.sixdegreesoffrancisbacon.com/relationships/#{id}.json&quot;, :query=&amp;gt; {:id =&amp;gt; id, :output =&amp;gt; &quot;json&quot;})
    end

end

class Personalizer
    include HTTParty
    format :json

    def self.find_by_id(num)
        get(&quot;http://www.sixdegreesoffrancisbacon.com/people/#{num}.json&quot;, :query=&amp;gt; {:num =&amp;gt; num, :output =&amp;gt; &quot;json&quot;})
    end

end

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Next we create hashes that reference various JSON objects. In other words, we’re creating key value pair items. If you’ve worked with dictionaries in python, these are the same thing. If not, we’re essentially pulling the json object and morphing into a workable format.  I should note here that if you see Randomizer, I am pulling from the relationship url. If you see Personalizer, I am pulling from the people URL.&lt;/p&gt;

&lt;p&gt;The “Go Fish” bit is when you hit a null. go fish&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rel_id = Hash.new( Randomizer.find_by_id(&quot;#{randos[2]}&quot;).parsed_response{0}[&quot;id&quot;] )
p1 = Hash.new(Randomizer.find_by_id(&quot;#{randos[2]}&quot;).parsed_response{0}[&quot;person1_index&quot;] )
p2 = Hash.new(Randomizer.find_by_id(&quot;#{randos[2]}&quot;).parsed_response{0}[&quot;person2_index&quot;] )

hash = Hash.new(&quot;Go Fish&quot;)

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;So let’s break this down a bit :&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hash[&quot;rel&quot;] = Randomizer.find_by_id(&quot;#{randos[2]}&quot;).parsed_response{0}[&quot;type_certainty_list&quot;]

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;The hash I’ve created above pulls from the relationship url. It parses the response and looks for the json object “type_certainty_list”. This hash produces the certainty number and the relationship type. In the Francis Bacon and Anne Bacon example above, the certainty percentage was 100 and the relationship type was “Parent of”.&lt;/p&gt;

&lt;p&gt;We’ve created a few more hashes below. These all mostly from the people information p1 is the first person in the relationship, p2 is the second:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## Pulls from person id and outputs person information

hesh = Hash.new(&quot;Go Fish&quot;)
hesh[&quot;person1&quot;] = Personalizer.find_by_id(&quot;#{p1[2]}&quot;).parsed_response{0}[&quot;display_name&quot;]
hesh[&quot;person2&quot;] = Personalizer.find_by_id(&quot;#{p2[2]}&quot;).parsed_response{0}[&quot;display_name&quot;]
hesh[&quot;hist1&quot;] = Personalizer.find_by_id(&quot;#{p1[2]}&quot;).parsed_response{0}[&quot;historical_significance&quot;]
hesh[&quot;hist2&quot;] = Personalizer.find_by_id(&quot;#{p2[2]}&quot;).parsed_response{0}[&quot;historical_significance&quot;]
hesh[&quot;by1&quot;] = Personalizer.find_by_id(&quot;#{p1[2]}&quot;).parsed_response{0}[&quot;ext_birth_year&quot;]
hesh[&quot;by2&quot;] = Personalizer.find_by_id(&quot;#{p2[2]}&quot;).parsed_response{0}[&quot;ext_birth_year&quot;]
hesh[&quot;dy1&quot;] = Personalizer.find_by_id(&quot;#{p1[2]}&quot;).parsed_response{0}[&quot;ext_death_year&quot;]
hesh[&quot;dy2&quot;] = Personalizer.find_by_id(&quot;#{p2[2]}&quot;).parsed_response{0}[&quot;ext_death_year&quot;]
hesh[&quot;by1_T&quot;] = Personalizer.find_by_id(&quot;#{p1[2]}&quot;).parsed_response{0}[&quot;birth_year_type&quot;]
hesh[&quot;by2_T&quot;] = Personalizer.find_by_id(&quot;#{p2[2]}&quot;).parsed_response{0}[&quot;birth_year_type&quot;]
hesh[&quot;dy1_T&quot;] = Personalizer.find_by_id(&quot;#{p1[2]}&quot;).parsed_response{0}[&quot;death_year_type&quot;]
hesh[&quot;dy2_T&quot;] = Personalizer.find_by_id(&quot;#{p2[2]}&quot;).parsed_response{0}[&quot;death_year_type&quot;]

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Next we had to substitute date information. &lt;a href=&quot;https://twitter.com/jotis13&quot;&gt;Jessica Otis&lt;/a&gt; noted that there is quite a bit of uncertainty with our dates and I simply added that information in and swapped out the abbrevations:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
##six modifiers (“AF” “BF” “CA” “IN” “AF/IN” “BF/IN”)
##which are abbreviations for (“after” “before” “circa” “in” “after or in” and “before or in”).

replacements = [ [&quot;AF&quot;, &quot;after&quot;], [&quot;BF&quot;, &quot;before&quot;], [&quot;CA&quot;, &quot;circa&quot;], [&quot;IN&quot;, &quot;in&quot;], [&quot;AF/IN&quot;, &quot;after or in&quot;],
    [&quot;BF/IN&quot;, &quot;before or in&quot;] ]
replacements.each {|replacement| hesh[&quot;by1_T&quot;].gsub!(replacement[0], replacement[1])}

replacements = [ [&quot;AF&quot;, &quot;after&quot;], [&quot;BF&quot;, &quot;before&quot;], [&quot;CA&quot;, &quot;circa&quot;], [&quot;IN&quot;, &quot;in&quot;], [&quot;AF/IN&quot;, &quot;after or in&quot;],
    [&quot;BF/IN&quot;, &quot;before or in&quot;] ]
replacements.each {|replacement| hesh[&quot;by2_T&quot;].gsub!(replacement[0], replacement[1])}

replacements = [ [&quot;AF&quot;, &quot;after&quot;], [&quot;BF&quot;, &quot;before&quot;], [&quot;CA&quot;, &quot;circa&quot;], [&quot;IN&quot;, &quot;in&quot;], [&quot;AF/IN&quot;, &quot;after or in&quot;],
    [&quot;BF/IN&quot;, &quot;before or in&quot;] ]
replacements.each {|replacement| hesh[&quot;dy1_T&quot;].gsub!(replacement[0], replacement[1])}

replacements = [ [&quot;AF&quot;, &quot;after&quot;], [&quot;BF&quot;, &quot;before&quot;], [&quot;CA&quot;, &quot;circa&quot;], [&quot;IN&quot;, &quot;in&quot;], [&quot;AF/IN&quot;, &quot;after or in&quot;],
    [&quot;BF/IN&quot;, &quot;before or in&quot;] ]
replacements.each {|replacement| hesh[&quot;dy2_T&quot;].gsub!(replacement[0], replacement[1])}

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Next we construct the tweets from the hashes:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
option = Hash.new( &quot;The @6bacon community is &quot; + hash[&quot;rel&quot;][0][1].floor.to_s + &quot;% certain that &quot; + hesh[&quot;person1&quot;] + &quot; &quot; +
    hash[&quot;rel&quot;][0][2].downcase + &quot; &quot; + hesh[&quot;person2&quot;] + &quot;: http://www.sixdegreesoffrancisbacon.com/relationships/#{rel_id[0]}&quot; )

option1 = Hash.new( &quot;The @6bacon community is &quot; + hash[&quot;rel&quot;][0][1].floor.to_s + &quot;% certain that &quot; + hesh[&quot;person1&quot;] + &quot; was &quot; +
    hash[&quot;rel&quot;][0][2].downcase + &quot; &quot; + hesh[&quot;person2&quot;] + &quot;: http://www.sixdegreesoffrancisbacon.com/relationships/#{rel_id[0]}&quot; )

option2 = Hash.new( &quot;The @6bacon community is &quot; + hash[&quot;rel&quot;][0][1].floor.to_s + &quot;% certain that &quot; + hesh[&quot;person1&quot;] + &quot; was a &quot; +
    hash[&quot;rel&quot;][0][2].downcase + &quot; &quot; + hesh[&quot;person2&quot;] + &quot;: http://www.sixdegreesoffrancisbacon.com/relationships/#{rel_id[0]}&quot; )

option3 = Hash.new( &quot;The @6bacon community is &quot; + hash[&quot;rel&quot;][0][1].floor.to_s + &quot;% certain that &quot; + hesh[&quot;person1&quot;] + &quot; was an &quot; +
    hash[&quot;rel&quot;][0][2].downcase + &quot; &quot; + hesh[&quot;person2&quot;] + &quot;: http://www.sixdegreesoffrancisbacon.com/relationships/#{rel_id[0]}&quot; )

h = Hash.new(&quot;I am a bot. Please see more at : www.sixdegreesoffrancisbacon.com/&quot;)
h[&quot;who1&quot;] = ( hesh[&quot;person1&quot;] + &quot;, &quot; + hesh[&quot;hist1&quot;] + &quot;, was born &quot; + hesh[&quot;by1_T&quot;] + &quot; &quot; + hesh[&quot;by1&quot;].to_s +
    &quot; and died &quot; + hesh[&quot;dy1_T&quot;] + &quot; &quot; + hesh[&quot;dy1&quot;].to_s + &quot;: http://www.sixdegreesoffrancisbacon.com/people/#{p1[0]}&quot; )
h[&quot;who2&quot;] = ( hesh[&quot;person2&quot;] + &quot;, &quot; + hesh[&quot;hist2&quot;] + &quot;, was born &quot; + hesh[&quot;by2_T&quot;] + &quot; &quot; + hesh[&quot;by2&quot;].to_s +
    &quot; and died &quot; + hesh[&quot;dy2_T&quot;] + &quot; &quot; + hesh[&quot;dy2&quot;].to_s + &quot;: http://www.sixdegreesoffrancisbacon.com/people/#{p2[0]}&quot; )

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Finally there is some code that is still in progress which is hosted on &lt;a href=&quot;https://github.com/danieljohnevans/6bacon_bot&quot;&gt;my github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I should also note that the bot is freely hosted on heroku and updates once an hour.&lt;/p&gt;

&lt;p&gt;To do:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Get the bot to respond to users&lt;/li&gt;
  &lt;li&gt;Pull from something other than first relationship.&lt;/li&gt;
  &lt;li&gt;Focus on particular demographics / time periods / locations / groups.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;what-ive-learned&quot;&gt;What I’ve Learned&lt;/h3&gt;

&lt;p&gt;To be honest I’ve been surprised with the conversation this bot has generated. I think it has raised important questions about certainty in historical data, open data and what role self-exposing error should play in a project. I’ll let more qualified persons than myself discuss those points.&lt;/p&gt;

&lt;p&gt;However, I am pleased with the bot and where it is going. I’ve learned quite a bit and would like to continue adding features to it so long as it continues to spark discussion.&lt;/p&gt;

&lt;p&gt;DJE&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Blog Reboot</title>
   <link href="http://danieljohnevans.github.io/about/reboot/2016/08/03/blog-reboot/"/>
   <updated>2016-08-03T13:09:50-04:00</updated>
   <id>http://danieljohnevans.github.io/about/reboot/2016/08/03/blog-reboot</id>
   <content type="html">&lt;p&gt;Hello again.&lt;/p&gt;

&lt;p&gt;It’s been over a year and here I am reviving this site from the dead.&lt;/p&gt;

&lt;p&gt;These posts going forward are to act as notebook entries rather than blog posts. They are mean to function as a way for me to document my work and interests so that I (and hopefully others) may return to it at a later date. Additionally, it will provide me (and hopefully others) with documentation for working through problems or areas of interest.&lt;/p&gt;

&lt;p&gt;As such, I may work through various tutorials, books, or problems on here. Similarly, I may post and document my code as I work through issues. I may in the future put up other writing materials.&lt;/p&gt;

&lt;p&gt;Anyway I want to keep this short so I can actually get to the hacks.&lt;/p&gt;

&lt;p&gt;DJE&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>A New Nation Votes</title>
   <link href="http://danieljohnevans.github.io/about/nnv/r/node/2015/07/28/nnv-plotting/"/>
   <updated>2015-07-28T13:09:50-04:00</updated>
   <id>http://danieljohnevans.github.io/about/nnv/r/node/2015/07/28/nnv-plotting</id>
   <content type="html">&lt;p&gt;It’s been a busy summer so you’ll have to bear with me here if this post runs a bit long.&lt;/p&gt;

&lt;p&gt;I’m fortunate enough to have some professional contact with &lt;a href=&quot;http://www.twitter.com/mollyhardy&quot;&gt;Molly Hardy&lt;/a&gt;, the Digital Humanities Curator at the American Antiquarian Society, and expressed interest in working with some of the AAS’ vast holdings. She in turn graciously introduced me to a New Nation Votes datasets and provided insight into their history, depth, and limitations.&lt;/p&gt;

&lt;p&gt;A New Nation Votes project is coordinated by both the AAS and Tufts University through a grant from the National Endowment for the Humanities. The project seeks to capture and index voting records from 1787 to 1825. It is a mammoth undertaking.&lt;/p&gt;

&lt;p&gt;Although, I’m by no means a scholar of early US history, I was captivated. &lt;a href=&quot;http://www.neh.gov/humanities/2008/januaryfebruary/feature/the-orphan-scholar&quot;&gt;Phil Lampi&lt;/a&gt; has devoted the better part of his life to diligently cataloguing and organizing this collection. The AAS and Tufts in turn are in the process of producing an amazing set of voting records that provide a unique insight into the history of the Early Republic. While I’m lucky to be playing with this data, it really deserves a much deeper dive than I can provide.&lt;/p&gt;

&lt;p&gt;For example, I’m struggling with changing town names and had to limit my scope to New England and New York.  More on that in a moment. First let’s talk about how I did what I did.&lt;/p&gt;

&lt;p&gt;I approached this project with the goal of emulating the work that has been done with other GIS programs by both graduate and undergraduate students at WPI. Keeping that in mind, I originally hoped to plot county level geographic data points using ArcGIS (primarily because it is free) and intended to port them over to angular/d3.js for a nice visualization.&lt;/p&gt;

&lt;p&gt;Luckily, I stumbled upon Lincoln Mullen’s amazing resource &lt;a href=&quot;http://lincolnmullen.com/projects/dh-r/&quot;&gt;&lt;em&gt;Digital History Methods in R&lt;/em&gt;&lt;/a&gt;. If you’re looking for an introduction to R with a digital history/DH bent, I cannot recommend this book enough. I use R in my current position as an analyst and familiarized myself with the textual analyses CRAN packages last winter/spring so the progression to using R for its GIS capabilities was an easy transition. Prof. Mullen provides a strong foundation in using R including data manipulation, munging, and a great section on plotting. One of my favorite things about his book, however, is that he frames it all within the realm of history. Furthermore, his CRAN packages provide another strong resource for those looking for large, relatively clean historic datasets. I’ve been using his  &lt;a href=&quot;https://cran.r-project.org/web/packages/USAboundaries/USAboundaries.pdf&quot;&gt;USAboundaries&lt;/a&gt; library to map historic county lines.&lt;/p&gt;

&lt;p&gt;To begin I headed over to the &lt;a href=&quot;http://elections.lib.tufts.edu/&quot;&gt;NNV site&lt;/a&gt; and &lt;a href=&quot;http://dl.tufts.edu/election_datasets&quot;&gt;downloaded&lt;/a&gt; the specific files I wanted to work with – in this case ME, NH, VT, MA, RI, CT, and NY. I placed each of those files in a folder.&lt;/p&gt;

&lt;p&gt;I used the following packages in this exercise:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;library(ggmap)   
library(dplyr)   
library(tidyr)   
library(rgdal)   
library(ggplot2)   
library(USAboundaries)    
library(stringr)    
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Please note that USAboundaries needs to be installed using &lt;a href=&quot;https://github.com/hadley/devtools&quot;&gt;devtools&lt;/a&gt;, however, this is a fairly straightforward process and following the directions on the github should yield the desired results.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;Cleaning and Geocoding&lt;/h2&gt;

&lt;p&gt;With all packages installed, we can begin reading data into R. First set the working directory to your folder with the NNV data and save it as wd. Next, list each file in the folder and read each file in with a ‘tab’ deliminating columns. Finally, combine each file based on column headings.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wd &amp;lt;- setwd(&quot;####&quot;)
load_data &amp;lt;- function(path) {
    files &amp;lt;- list.files( path = wd)
    tables &amp;lt;- lapply(files, read.csv, sep = &quot;\t&quot;)
    do.call(rbind, tables)
}

mapdata &amp;lt;- load_data(wd)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Because the column headings are consistent across each of our data sets, R has no problem combining them into one master file. That being said, it helps to perform ‘head(mapdata)’ or ‘str(mapdata)’ to check double and triple check your work. With less clean data you may need to go back and edit a heading or falsely insert a column.&lt;/p&gt;

&lt;p&gt;Now that we have our masterfile read in (renamed to ‘mapdata’), we need to isolate the unique towns and cities and begin geocoding. To do this we first need to concatinate our town column and state column. This way we’ll avoid confusing Google; rather than looking up &lt;code class=&quot;highlighter-rouge&quot;&gt;Mexico&lt;/code&gt; and ending up with the latitude and longitude of Mexico City, we’ll specify &lt;code class=&quot;highlighter-rouge&quot;&gt;Mexico, Maine&lt;/code&gt; and hopefully get somewhere in Maine.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;towns &amp;lt;- str_c(mapdata$Town, mapdata$State, sep= &quot;, &quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Typing &lt;code class=&quot;highlighter-rouge&quot;&gt;towns&lt;/code&gt; into the R command line will give you a list of every town and state within the master file. This is too much. We only want the unique town names.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;allT &amp;lt;- (str_trim(unique(sort(towns), stringsAsFactors=FALSE)))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;allT&lt;/code&gt; will give us a sorted list. Now we are getting somewhere and are almost ready to begin geocoding. However, closely looking at the list reveals that the first nine variables are false entries. Removing those will give us a clean data set to work with:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;allTowns &amp;lt;- allT[9:2372]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;With that, the fun begins. Lincoln Mullen’s textbook provides an excellent introduction to incorporating Google’s geocoding API into R using the &lt;code class=&quot;highlighter-rouge&quot;&gt;ggmap&lt;/code&gt; package and I make use of this as well. The free version of this service limits you to 2500 downloads a day. In order to work with the API you must first convert your vector to a data frame. This will allow you to give it an index and ultimately append coordinates to it. Once converted to a data frame, we’ll geocode the data. Please note that this will take some time so go make a cup of coffee. After all data is geocoded, we then bind both data frames together and rename it to &lt;code class=&quot;highlighter-rouge&quot;&gt;allLocations&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;firstTowns &amp;lt;- data.frame (towns = c (allTowns),stringsAsFactors = FALSE)
geocodedTowns &amp;lt;- geocode(firstTowns$towns)
firstTowns &amp;lt;- cbind(firstTowns, geocodedTowns)
allLocations &amp;lt;- firstTowns
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;If you’re feeling lazy, I’ve provided a messy data set on my &lt;a href=&quot;https://github.com/danieljohnevans/NNV-Geocoding&quot;&gt;github&lt;/a&gt;. This is for every town, city and location. Please feel free to fork, update, etc. I will be beyond elated if I receive any interest in collaboration or contributions.&lt;/p&gt;

&lt;p&gt;I’ve also provided a cleaner data set for the New England and NY data set we’re currently working with there as well.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;Merging&lt;/h2&gt;

&lt;p&gt;Still with me? For those of you playing along at home, we have a quick bit of column renaming to do before we can plot out points. This will help us out further on down the road when we merge our data back with the masterfile ‘mapdata’. Additionally, we’ll want to merge our &lt;code class=&quot;highlighter-rouge&quot;&gt;allLocations&lt;/code&gt; data frame back to our mapdata master file so that we’ll be easily able to call on those files later. Finally, if you haven’t already, now would be a great time to write out your various files to .CSVs in the event of a computer or program crash.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;write.csv(firstTowns, file= &quot;your/file/here&quot;)
str(mapdata)

##merge allLocations to mapdata
mapdata &amp;lt;- mutate(mapdata,
    location = str_c(mapdata$Town, mapdata$City))
mapdata &amp;lt;-  mutate(mapdata,
    location = str_c(mapdata$location, mapdata$State, sep= &quot;, &quot;))

str(allLocations)
names(allLocations) &amp;lt;- c(&quot;x&quot;, &quot;location&quot;, &quot;lon&quot;, &quot;lat&quot;)
str(allLocations)

mapdata_merged &amp;lt;- left_join(mapdata, allLocations, by = c(&quot;location&quot; = &quot;location&quot;))

str(mapdata_merged)

write.csv(mapdata_merged, file= &quot;your/file/here&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;hr /&gt;

&lt;h2&gt;Mapping&lt;/h2&gt;

&lt;p&gt;Now that your coordinates are saved, you can easily import them at your leisure without going through the multitude of data munging steps. With that said, it’s finally time to see how things look on a map. A quick note on plotting, the CRAN package &lt;code class=&quot;highlighter-rouge&quot;&gt;ggplot2&lt;/code&gt; is the most efficient charting tool in R. Charts and graphs in R are finicky. I really can’t recommend anything other than to just keep grinding away at them. In the case of maps, I recommend taking a look at Lincoln Mullen’s write up on GIS maps. My code is below:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;USA &amp;lt;- c(&quot;Connecticut&quot;,&quot;Maine&quot;, &quot;Massachusetts&quot;, &quot;New Hampshire&quot;,  
    &quot;New York&quot;, &quot;Rhode Island&quot;, &quot;Vermont&quot;)  
map &amp;lt;- us_boundaries(as.Date(&quot;1825-03-15&quot;), type = &quot;county&quot;, state = USA)
usMap &amp;lt;- ggplot() +  geom_polygon(data=map, aes(x=long, y=lat, group=group))
usMap +
    ggtitle(&quot;County Boundaries on March 15, 1825&quot;) +
    geom_text(data = allLocations, aes(x = lon, y = lat, label = location),  
    color=&quot;gray&quot;,
    vjust = -1,
    size = 4) +
    geom_point(data = allLocations, aes(x = lon, y = lat), color= &quot;red&quot;) +
    theme(legend.position = &quot;bottom&quot; ) +
    theme_minimal()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;If you’ve followed along thus far, importing all packages and geocoding everything, this is what you’ll get:
&lt;img src=&quot;/assets/Rplot_raw.png&quot; alt=&quot;NE MAP RAW&quot; class=&quot;center-image responsive-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Most likely this is not what you were hoping to see. I’ve never plotted anything in R and got it right my first time. In this case the changing and variable names of US towns are the problem. Admittedly, the data cleanup and georectification part of this process is taking longer than expected. In many ways, I’ve chosen to limit my scope to New England and New York due to the overwhelming number of towns in a NNV. I think working with a smaller data set initally and expanding my scope outward after deployment will help to isolate many of the issues I’m facing.&lt;/p&gt;

&lt;p&gt;Take for instance the MA/ME split in 1820. All Maine towns in the NNV data set rightfully fell under the jurisdiction of Massachusetts prior to 1820. Ergo, many of the initial errors on the map above can be blamed on Google maps getting confused by places like &lt;code class=&quot;highlighter-rouge&quot;&gt;Denmark, Massachusetts&lt;/code&gt;. Instead it should be looking for &lt;code class=&quot;highlighter-rouge&quot;&gt;Denmark, Maine&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Spelling variations pose another problem. These anomolies are more difficult for me to find and are oftentimes only discovered via manual checks. An obvious example of this is &lt;code class=&quot;highlighter-rouge&quot;&gt;Chili, New York&lt;/code&gt; verses &lt;code class=&quot;highlighter-rouge&quot;&gt;Chile, New York&lt;/code&gt;. This registers as two radically different locations for Google’s API. However, sometimes Google will plot a variable in another county or state and this isn’t immediately apparent.&lt;/p&gt;

&lt;p&gt;Finally, I’m struggling with towns simply disappearing from the historical record. Take for example &lt;code class=&quot;highlighter-rouge&quot;&gt;Phillipe, New York&lt;/code&gt; which, according to the master file is located in Dutchess County. It appears that it was a part of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Philipse_Patent&quot;&gt;Philipse Patent&lt;/a&gt; and is probably a misspelling. What was once Philipse, Dutchess County, New York was incorporated into &lt;a href=&quot;http://www.putnamcountyny.com/countyhistorian/boundary-changes/&quot;&gt;Fishkill, New York&lt;/a&gt; after the Revolutionary War and I’ve georectified to make up for the loss.&lt;/p&gt;

&lt;p&gt;These investigations take time and the going is slow. A few miscellaneous notes:&lt;/p&gt;

&lt;ul&gt;For whatever reason, CT and RI seem to have relatively static names. I&#39;ve made few, if any, georectifications due to name changes or spelling variations.&lt;/ul&gt;
&lt;ul&gt;Most of my time is devoted to trying to figure out Upstate NY&#39;s complicated village system. This information has been difficult to find. &lt;/ul&gt;
&lt;ul&gt;This contrasts sharply with ME, NE and VT. They seem to have devoted historians/Wikipedians. Town name changes are diligently noted on Wikipedia pages or easily findable on the web.&lt;/ul&gt;

&lt;p&gt;My current iteration plots out to:
&lt;img src=&quot;/assets/Rplot_clean.png&quot; alt=&quot;NE MAP Cleaner&quot; class=&quot;center-image responsive-image&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;I’m about a third through the list and hope to finish by the end of the summer. My ultimate goal is to reintroduce these georectified locations into the original NNV dataset and plot historic voting records by town and county on an angular/d3 website. I’d like to then compare those numbers against US Census records to examine voter turnout per town per election. I’ll pull the OCR for this data using tesseract as I haven’t yet seen an API that documents town level census data back that far. That being said, it’d be great to receive feedback on any part of this process so please feel free to reach out.&lt;/p&gt;

&lt;p&gt;The complete code used in this project can be found &lt;a href=&quot;https://github.com/danieljohnevans/electionsNE&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A quick note about the backend of this site - I’ve been playing around with node.js. Earlier this summer, I redployed this site under the yeoman-jekyllrb framework but have since reverted to my jekyll bootstrap framework. As a task runner, Grunt.js is giving me more problems than it’s solving. Every time I try to deploy using it, I receive a litany of error messages. I know I’ll return to this in the coming weeks but my initial thought is that the problem may be in grunt and I may need to look at gulp.js instead.&lt;/p&gt;

&lt;p&gt;More soon.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Professional Blogging</title>
   <link href="http://danieljohnevans.github.io/r/dh/2015/06/23/professional-blogging/"/>
   <updated>2015-06-23T13:09:50-04:00</updated>
   <id>http://danieljohnevans.github.io/r/dh/2015/06/23/professional-blogging</id>
   <content type="html">&lt;p&gt;Earlier this spring I worked with Wendy Woloson, an Assistant Professor of History at Rutgers Camden, to geocode and map the locations of EBSCO’s &lt;em&gt;American Antiquarian Society Periodicals Collections.&lt;/em&gt; She was preparing to give a presentation at the Research Society of American Periodicals conference in May of 2015. Those visualizations, in conjunction with a social network analysis markup I created last fall, became the foundation for a blog post I wrote earlier this month.&lt;/p&gt;

&lt;p&gt;It was recently featured on EBSCO’s marketing page. In the post, I broadly speak about the digital humanities. I also touch upon some of text and data mining projects I’m currently working on.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.ebsco.com/blog/article/ebsco-and-digital-humanities-data-and-text-mining&quot;&gt;Check it out here.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Because the images are small on the blog, I’ve included a higher quality version of Massachusetts below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/aas_MA.png&quot; alt=&quot;NE MAP&quot; class=&quot;center-image responsive-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Please feel free to reach out if you’d like a larger copy of the entire United States or would like a bit more background regarding the social network analysis.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Technical Notes</title>
   <link href="http://danieljohnevans.github.io/about/jekyll/bootstrap/2015/04/20/technical-notes/"/>
   <updated>2015-04-20T13:09:50-04:00</updated>
   <id>http://danieljohnevans.github.io/about/jekyll/bootstrap/2015/04/20/technical-notes</id>
   <content type="html">&lt;body&gt;
&lt;p&gt;So I feel that I should add a few technical notes to this blog now that this is off the ground. As alluded to in my first post a few months ago, this site has gone through a few different iterations. It began with a Wordpress.org account. However, that was quickly abandoned when I realized that for my purposes the features of a full blown WAMP/MAMP environment weren&#39;t necessary.&lt;/p&gt;

&lt;p&gt;Instead I needed something lighter, easier to maintain and, in my opinion, a lot more fun to customize. I had heard that Bootstrap provided many of those features. So while Boston slowly dug itself out earlier this year, I spent some time building out a single page site using the Bootstrap API. In fact, some of what you see on the &quot;about&quot; page comes from that exercise. &lt;/p&gt;

&lt;p&gt; However, once I discovered &lt;a href=&quot;http://jekyllbootstrap.com/&quot;&gt;Jekyll Bootstrap&lt;/a&gt; earlier this weekend, I knew I was sold on a framework. Jekyll Bootstrap allows an end user to quickly build out a blogging platform. It is lightweight, easy to deploy, fully customizable and, as an added bonus, hosts freely on Github. I&#39;ve gotten a website off the ground within the past few days. &lt;/p&gt;

&lt;p&gt;Now I can focus on blogging rather than maintaining a website.&lt;/p&gt;

&lt;p&gt;stay tuned.&lt;/p&gt;

&lt;/body&gt;

</content>
 </entry>
 
 <entry>
   <title>A Year in Review</title>
   <link href="http://danieljohnevans.github.io/about/2015/01/02/books-in-review/"/>
   <updated>2015-01-02T12:09:50-05:00</updated>
   <id>http://danieljohnevans.github.io/about/2015/01/02/books-in-review</id>
   <content type="html">&lt;p&gt;I moved to Somerville from the North Shore in September. While I’m still working on the North Shore, my commute has invariably risen from a seemingly quick 30 minute car ride to a 1.5 to 2 hour subway-to-commuter-rail marathon.&lt;/p&gt;

&lt;p&gt;I’ve started reading. A lot.&lt;/p&gt;

&lt;p&gt;Since January 2014, I’ve read:&lt;/p&gt;

&lt;body&gt;
    &lt;table class=&quot;table table-condensed table-hover&quot;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;#&lt;/th&gt;
            &lt;th&gt;Author&lt;/th&gt;
            &lt;th&gt;Work&lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
    &lt;tr&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;Bukowski, Charles &lt;/td&gt;
        &lt;td&gt;&lt;i&gt;Hot Water Music&lt;/i&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;2&lt;/td&gt;
        &lt;td&gt;Carver, Raymond &lt;/td&gt;
        &lt;td&gt;&lt;i&gt;Cathedral&lt;/i&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;3&lt;/td&gt;
        &lt;td&gt;Carver, Raymond &lt;/td&gt;
        &lt;td&gt;&lt;i&gt;What We Talk About When We Talk About Love&lt;/i&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;4&lt;/td&gt;
        &lt;td&gt;Dumas, Alexandre  &lt;/td&gt;
        &lt;td&gt;&lt;i&gt;The Count of Monte Cristo&lt;/i&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;5&lt;/td&gt;
        &lt;td&gt;Ellis, Bret Easton  &lt;/td&gt;
        &lt;td&gt;&lt;i&gt;American Psycho&lt;/i&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;6&lt;/td&gt;
        &lt;td&gt;Fariña, Richard &lt;/td&gt;
        &lt;td&gt;&lt;i&gt;Been Down So Long it Looks Like Up to Me&lt;/i&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;7&lt;/td&gt;
        &lt;td&gt;Hajdu, David  &lt;/td&gt;
        &lt;td&gt;&lt;i&gt;Positively 4th Street: The Lives and Times of Joan Baez, Bob Dylan, Mimi Baez Fariña, and Richard Fariña&lt;/i&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;8&lt;/td&gt;
        &lt;td&gt;Hemingway, Ernest  &lt;/td&gt;
        &lt;td&gt;&lt;i&gt;The Old Man and the Sea&lt;/i&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;9&lt;/td&gt;
        &lt;td&gt;Hemingway, Ernest &lt;/td&gt;
        &lt;td&gt;&lt;i&gt;The Sun Also Rises&lt;/i&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;10&lt;/td&gt;
        &lt;td&gt;Ilgunas, Ken  &lt;/td&gt;
        &lt;td&gt;&lt;i&gt;Walden on Wheels: On the Open Road from Debt to Freedom&lt;/i&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;11&lt;/td&gt;
        &lt;td&gt;Johnson, Denis  &lt;/td&gt;
        &lt;td&gt;&lt;i&gt;Tree of Smoke&lt;/i&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;12&lt;/td&gt;
        &lt;td&gt;Kerouac, Jack  &lt;/td&gt;
        &lt;td&gt;&lt;i&gt;Tristessa&lt;/i&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;13&lt;/td&gt;
        &lt;td&gt;Lahiri, Jhumpa  &lt;/td&gt;
        &lt;td&gt;&lt;i&gt;The Namesake&lt;/i&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;14&lt;/td&gt;
        &lt;td&gt;Lampedusa, Giuseppe Tomasi di &lt;/td&gt;
        &lt;td&gt;&lt;i&gt;The Leopard&lt;/i&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;15&lt;/td&gt;
        &lt;td&gt;McCarthy, Cormac  &lt;/td&gt;
        &lt;td&gt;&lt;i&gt;Blood Meridian, or the Evening Redness in the West&lt;/i&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;16&lt;/td&gt;
        &lt;td&gt;McMurtry, Larry   &lt;/td&gt;
        &lt;td&gt;&lt;i&gt;Lonesome Dove&lt;/i&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;17&lt;/td&gt;
        &lt;td&gt;McCarthy, Cormac &lt;/td&gt;
        &lt;td&gt;&lt;i&gt;Blood Meridian, or the Evening Redness in the West&lt;/i&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;18&lt;/td&gt;
        &lt;td&gt;Millard, Candice &lt;/td&gt;
        &lt;td&gt;&lt;i&gt;The River of Doubt: Theodore Roosevelt’s Darkest Journey&lt;/i&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;19&lt;/td&gt;
        &lt;td&gt;Millard, Candice &lt;/td&gt;
        &lt;td&gt;&lt;i&gt;Destiny of the Republic: A Tale of Madness, Medicine and the Murder of a President&lt;/i&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;20&lt;/td&gt;
        &lt;td&gt;Mitchell, Joseph &lt;/td&gt;
        &lt;td&gt;&lt;i&gt;Up in the Old Hotel&lt;/i&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;21&lt;/td&gt;
        &lt;td&gt;Munro, Alice &lt;/td&gt;
        &lt;td&gt;&lt;i&gt;Dear Life: Stories&lt;/i&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;22&lt;/td&gt;
        &lt;td&gt;Munro, Alice &lt;/td&gt;
        &lt;td&gt;&lt;i&gt;Runaway&lt;/i&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;23&lt;/td&gt;
        &lt;td&gt;Piketty, Thomas &lt;/td&gt;
        &lt;td&gt;&lt;i&gt;Capital in the Twenty-First Century&lt;/i&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;24&lt;/td&gt;
        &lt;td&gt;Salter, James &lt;/td&gt;
        &lt;td&gt;&lt;i&gt;A Sport and a Pastime&lt;/i&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;25&lt;/td&gt;
        &lt;td&gt;Saunders, George  &lt;/td&gt;
        &lt;td&gt;&lt;i&gt;Tenth of December&lt;/i&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;26&lt;/td&gt;
        &lt;td&gt;Saunders, George &lt;/td&gt;
        &lt;td&gt;&lt;i&gt;CivilWarLand in Bad Decline&lt;/i&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;27&lt;/td&gt;
        &lt;td&gt;Tower, Wells &lt;/td&gt;
        &lt;td&gt;&lt;i&gt;Everything Ravaged, Everything Burned&lt;/i&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;28&lt;/td&gt;
        &lt;td&gt;Wood, Pamela &lt;/td&gt;
        &lt;td&gt;&lt;i&gt;The Salt Book: Lobstering, Sea Moss Pudding, Stone Walls, Rum Running, Maple Syrup, Snowshoes, and Other Yankee Doings&lt;/i&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;29&lt;/td&gt;
        &lt;td&gt;Zweig, Stefan &lt;/td&gt;
        &lt;td&gt;&lt;i&gt;Beware of Pity&lt;/i&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/tbody&gt;
    &lt;/table&gt;
&lt;/body&gt;

&lt;p&gt;To be honest, I read most of these books this fall. I loved Candice Millard’s writing. Her ideas and sentences flowed seamlessly. I’ve recommended her to a few friends and even those who aren’t usually fans of popular history are really blown away by her captivating style. I was pleasantly surprised to see her interviewed on Ken Burns’ Roosevelts.&lt;/p&gt;

&lt;p&gt;I feel that I should also note that I’ve gone on a bit of a short story kick. I struggled to put down the works of Carver, Munro, and Saunders.&lt;/p&gt;

&lt;p&gt;Not recorded here are the countless times I spent picking up, and putting down, &lt;code class=&quot;highlighter-rouge&quot;&gt;Infinite Jest&lt;/code&gt;. I just can’t seem to commit to that book. I feel like I’ll pick it up and get into it only to read a review or receive a recommendation and drop it for something more approachable. There’s so many other things I’m dying to read. Maybe I’ll go on a media blackout in 2015 and just force myself through it.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Welcome</title>
   <link href="http://danieljohnevans.github.io/about/dh/2015/01/01/welcome/"/>
   <updated>2015-01-01T12:09:50-05:00</updated>
   <id>http://danieljohnevans.github.io/about/dh/2015/01/01/welcome</id>
   <content type="html">&lt;p&gt;This site has been in the making for awhile now. Actually it lived and died in Wordpress for a brief period last fall and is finally live via Jekyll.&lt;/p&gt;

&lt;p&gt;I’m using this blog to record my thoughts and projects specifically as they relate to my explorations in digital history. I think it’ll be useful to have a place to write down my blunders and (hopefully) get feedback from the larger DH community.&lt;/p&gt;

&lt;p&gt;All the best –&lt;/p&gt;

</content>
 </entry>
 
 
</feed>
